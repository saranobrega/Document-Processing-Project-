{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ba03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import structlog\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22c7df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#workist functions for converting data\n",
    "\n",
    "logger = structlog.getLogger(__name__)\n",
    "\n",
    "\n",
    "def convert_nested_to_flat_bbox(bb):\n",
    "    \"\"\"\n",
    "    bb : [[X1,Y1],[X2,Y2]] BBOX\n",
    "    return: [X1,Y1,X2,Y2] BBOX\n",
    "    \"\"\"\n",
    "    return [bb[0][0], bb[0][1], bb[1][0], bb[1][1]]\n",
    "\n",
    "def convert_relative_to_abs_bbox(bbox, image_width, image_height):\n",
    "    \"\"\"\n",
    "    Converts a given bbox with relative coordinates to absolute coordinates\n",
    "    \"\"\"\n",
    "    return [\n",
    "        [bbox[0][0] * image_width, bbox[0][1] * image_height],\n",
    "        [bbox[1][0] * image_width, bbox[1][1] * image_height],\n",
    "    ]\n",
    "\n",
    "def convert_page_to_annotation_page(page, categories):\n",
    "    document_id = page[\"file_name\"].split(\"_\")[0]\n",
    "    url = f\"images/{page['file_name']}\"\n",
    "    return {\n",
    "        \"width\": page[\"page_width\"],\n",
    "        \"height\": page[\"page_height\"],\n",
    "        \"file_name\": url,\n",
    "        \"url\": url,\n",
    "        \"operator_job_id\": document_id,\n",
    "        \"image_id\": url,\n",
    "        \"annotations\": [\n",
    "            {\n",
    "                \"class\": categories.index(area[\"class\"]),\n",
    "                \"bbox\": convert_nested_to_flat_bbox(\n",
    "                    convert_relative_to_abs_bbox(area[\"bbox\"], page[\"page_width\"], page[\"page_height\"])\n",
    "                ),\n",
    "                \"bbox_mode\": 0,\n",
    "                \"category_id\": categories.index(area[\"class\"]),\n",
    "            }\n",
    "            for area in page[\"areas\"]\n",
    "        ],\n",
    "    }\n",
    "\n",
    "def load_data_from_disc(data_dir):\n",
    "    categories = [\"table\", \"footer\", \"header\", \"order_line_item_header\", \"order_line_item\"]\n",
    "    all_annotations = []\n",
    "    json_data_dir = f\"{data_dir}/training_data_json\"\n",
    "    for file in os.listdir(json_data_dir):\n",
    "        if file.endswith(\".json\"):\n",
    "            with open(os.path.join(json_data_dir, file)) as f:\n",
    "                page = json.load(f)\n",
    "                try:\n",
    "                    all_annotations.append(convert_page_to_annotation_page(page, categories))\n",
    "                except Exception as e:\n",
    "                    logger.error(\n",
    "                        f\"Could not convert page (id: {page['file_name'].split('_')[0]}) to annotation page:\\n {e}\"\n",
    "                    )\n",
    "\n",
    "    return all_annotations, categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53a4f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set root directory where the training_data_json is located\n",
    "\n",
    "root_dir = 'training_data'\n",
    "# all_annotations, categories=load_data_from_disc(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e467e70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all_annotations and categories loaded from disc \n",
    "\n",
    "# with open(str(data_dir) + '/all_annotations.json','w') as json_file:\n",
    "#     json.dump(all_annotations, json_file)\n",
    "# with open(str(data_dir) + '/categories.json','w') as json_file:\n",
    "#     json.dump(categories, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0216d347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all_annotations and categories\n",
    "\n",
    "with open(root_dir + '/all_annotations.json', 'r') as file:\n",
    "     all_annotations = json.load(file)\n",
    "with open(os.path.join(root_dir,'categories.json'), 'r') as file:\n",
    "     categories = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15877c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['table', 'footer', 'header', 'order_line_item_header', 'order_line_item']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check categories\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f990cf6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65210"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check number of samples \n",
    "len(all_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebd4c96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function that takes n samples from all annotations and does the train/val/test split\n",
    "\n",
    "def n_samples_dataset_split(all_annotations, n, val_set_size, test_set_size):\n",
    "    \n",
    "    n_random_annotations = random.sample(all_annotations, n)\n",
    "    \n",
    "    print('Created a dataset containing {0} samples out of a larger dataset with {1} samples!'.\n",
    "          format(len(n_random_annotations),len(all_annotations)))\n",
    "\n",
    "    torch.manual_seed(1)\n",
    "    indices = torch.randperm(len(n_random_annotations)).tolist()\n",
    "    train_ind, test_ind = train_test_split(indices, test_size=test_set_size)\n",
    "    train_ind, val_ind = train_test_split(train_ind, test_size=val_set_size)\n",
    "\n",
    "    train_annotations = [n_random_annotations[i] for i in train_ind]\n",
    "    val_annotations = [n_random_annotations[i] for i in val_ind]\n",
    "    test_annotations = [n_random_annotations[i] for i in test_ind]\n",
    "    \n",
    "    print('-----------------------')\n",
    "    print('Train set size: ', len(train_annotations))\n",
    "    print('Val set size: ', len(val_annotations))\n",
    "    print('Test set size: ', len(test_annotations))\n",
    "    print('-----------------------')\n",
    "    \n",
    "    indices = [train_ind, val_ind, test_ind]\n",
    "    \n",
    "    return train_annotations, val_annotations, test_annotations, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfadc036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a dataset containing 500 samples out of a larger dataset with 65210 samples!\n",
      "-----------------------\n",
      "Train set size:  405\n",
      "Val set size:  45\n",
      "Test set size:  50\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "# set n to wanted number of samples, set validation and test size\n",
    "n = 500\n",
    "val_set_size=0.1 \n",
    "test_set_size=0.1\n",
    "train_page_annotation, val_page_annotation, test_page_annotation, indices = n_samples_dataset_split(all_annotations, \n",
    "                                                                                                    n, \n",
    "                                                                                                    val_set_size,\n",
    "                                                                                                    test_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eab1010",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save train, validation and test json in page annotation format\n",
    "\n",
    "with open(root_dir + '/train_page_annotation_{0}.json'.format(n),'w') as json_file:\n",
    "    json.dump(train_page_annotation, json_file)\n",
    "with open(root_dir  + '/val_page_annotation_{0}.json'.format(n),'w') as json_file:\n",
    "    json.dump(val_page_annotation, json_file)\n",
    "with open(root_dir + '/test_page_annotation_{0}.json'.format(n),'w') as json_file:\n",
    "    json.dump(test_page_annotation, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5adc472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-09 12:05.06 [info     ] Total of 152 pages skipped as no annotations were provided.\n",
      "2023-01-09 12:05.06 [info     ] Total of 13 annotations skipped as size was invalid (<5px).\n",
      "2023-01-09 12:05.06 [info     ] Total of 91159 annotations converted to the COCO format.\n",
      "[{'supercategory': 'orders', 'id': 1, 'name': 'table'}, {'supercategory': 'orders', 'id': 2, 'name': 'footer'}, {'supercategory': 'orders', 'id': 3, 'name': 'header'}, {'supercategory': 'orders', 'id': 4, 'name': 'order_line_item_header'}, {'supercategory': 'orders', 'id': 5, 'name': 'order_line_item'}]\n",
      "2023-01-09 12:05.06 [info     ] Total of 16 pages skipped as no annotations were provided.\n",
      "2023-01-09 12:05.06 [info     ] Total of 1 annotations skipped as size was invalid (<5px).\n",
      "2023-01-09 12:05.06 [info     ] Total of 9257 annotations converted to the COCO format.\n",
      "[{'supercategory': 'orders', 'id': 1, 'name': 'table'}, {'supercategory': 'orders', 'id': 2, 'name': 'footer'}, {'supercategory': 'orders', 'id': 3, 'name': 'header'}, {'supercategory': 'orders', 'id': 4, 'name': 'order_line_item_header'}, {'supercategory': 'orders', 'id': 5, 'name': 'order_line_item'}]\n",
      "2023-01-09 12:05.06 [info     ] Total of 18 pages skipped as no annotations were provided.\n",
      "2023-01-09 12:05.06 [info     ] Total of 2 annotations skipped as size was invalid (<5px).\n",
      "2023-01-09 12:05.06 [info     ] Total of 9209 annotations converted to the COCO format.\n",
      "[{'supercategory': 'orders', 'id': 1, 'name': 'table'}, {'supercategory': 'orders', 'id': 2, 'name': 'footer'}, {'supercategory': 'orders', 'id': 3, 'name': 'header'}, {'supercategory': 'orders', 'id': 4, 'name': 'order_line_item_header'}, {'supercategory': 'orders', 'id': 5, 'name': 'order_line_item'}]\n"
     ]
    }
   ],
   "source": [
    "from training.faster_rcnn.helper.coco_conv_1 import convert_to_coco\n",
    "\n",
    "#convert workist annotation to coco annotation using coco_conv_1 script that assigns indices [1,2,3,4,5] for classes\n",
    "\n",
    "train_coco_1_annotation = convert_to_coco(train_page_annotation, categories)\n",
    "val_coco_1_annotation = convert_to_coco(val_page_annotation, categories)\n",
    "test_coco_1_annotation = convert_to_coco(test_page_annotation, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9571836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-09 12:05.07 [info     ] Total of 152 pages skipped as no annotations were provided.\n",
      "2023-01-09 12:05.07 [info     ] Total of 13 annotations skipped as size was invalid (<5px).\n",
      "2023-01-09 12:05.07 [info     ] Total of 91159 annotations converted to the COCO format.\n",
      "[{'supercategory': 'orders', 'id': 0, 'name': 'table'}, {'supercategory': 'orders', 'id': 1, 'name': 'footer'}, {'supercategory': 'orders', 'id': 2, 'name': 'header'}, {'supercategory': 'orders', 'id': 3, 'name': 'order_line_item_header'}, {'supercategory': 'orders', 'id': 4, 'name': 'order_line_item'}]\n",
      "2023-01-09 12:05.07 [info     ] Total of 16 pages skipped as no annotations were provided.\n",
      "2023-01-09 12:05.07 [info     ] Total of 1 annotations skipped as size was invalid (<5px).\n",
      "2023-01-09 12:05.07 [info     ] Total of 9257 annotations converted to the COCO format.\n",
      "[{'supercategory': 'orders', 'id': 0, 'name': 'table'}, {'supercategory': 'orders', 'id': 1, 'name': 'footer'}, {'supercategory': 'orders', 'id': 2, 'name': 'header'}, {'supercategory': 'orders', 'id': 3, 'name': 'order_line_item_header'}, {'supercategory': 'orders', 'id': 4, 'name': 'order_line_item'}]\n",
      "2023-01-09 12:05.07 [info     ] Total of 18 pages skipped as no annotations were provided.\n",
      "2023-01-09 12:05.07 [info     ] Total of 2 annotations skipped as size was invalid (<5px).\n",
      "2023-01-09 12:05.07 [info     ] Total of 9209 annotations converted to the COCO format.\n",
      "[{'supercategory': 'orders', 'id': 0, 'name': 'table'}, {'supercategory': 'orders', 'id': 1, 'name': 'footer'}, {'supercategory': 'orders', 'id': 2, 'name': 'header'}, {'supercategory': 'orders', 'id': 3, 'name': 'order_line_item_header'}, {'supercategory': 'orders', 'id': 4, 'name': 'order_line_item'}]\n"
     ]
    }
   ],
   "source": [
    "from training.faster_rcnn.helper.coco_conv import convert_to_coco\n",
    "\n",
    "#convert workist annotation to coco annotation using coco_conv script that assigns indices [0,1,2,3,4] for classes\n",
    "\n",
    "train_coco_0_annotation = convert_to_coco(train_page_annotation, categories)\n",
    "val_coco_0_annotation = convert_to_coco(val_page_annotation, categories)\n",
    "test_coco_0_annotation = convert_to_coco(test_page_annotation, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71921b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save splitted page annotations and coco annotations in the same folder\n",
    "\n",
    "def save_annotations(root_dir, new_dir, n):\n",
    "    \n",
    "    #create new dir with n samples\n",
    "    cwd = os.getcwd()\n",
    "    dir = os.path.join(cwd, root_dir, new_dir)\n",
    "    \n",
    "    page_anno_dir = os.path.join(dir, \"page_annotation\")\n",
    "    coco_anno_0_dir = os.path.join(dir,\"coco_0_annotation\")\n",
    "    coco_anno_1_dir = os.path.join(dir,\"coco_1_annotation\")\n",
    "    \n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)\n",
    "    \n",
    "    if not os.path.exists(page_anno_dir):\n",
    "        os.mkdir(page_anno_dir)\n",
    "    if not os.path.exists(coco_anno_0_dir):\n",
    "        os.mkdir(coco_anno_0_dir)\n",
    "    if not os.path.exists(coco_anno_1_dir):\n",
    "        os.mkdir(coco_anno_1_dir)\n",
    "        \n",
    "    #save annotations in new dir\n",
    "    with open(page_anno_dir + '/train_page_annotation_{0}.json'.format(n),'w') as json_file:\n",
    "        json.dump(train_page_annotation, json_file)\n",
    "    with open(page_anno_dir  + '/val_page_annotation_{0}.json'.format(n),'w') as json_file:\n",
    "        json.dump(val_page_annotation, json_file)\n",
    "    with open(page_anno_dir + '/test_page_annotation_{0}.json'.format(n),'w') as json_file:\n",
    "        json.dump(test_page_annotation, json_file)\n",
    "\n",
    "    #save coco 0 annotations in new dir\n",
    "    with open(coco_anno_0_dir + '/train_coco_0_annotation_{0}.json'.format(n),'w') as json_file:\n",
    "        json.dump(train_coco_0_annotation, json_file)\n",
    "    with open(coco_anno_0_dir + '/val_coco_0_annotation_{0}.json'.format(n),'w') as json_file:\n",
    "        json.dump(val_coco_0_annotation, json_file)\n",
    "    with open(coco_anno_0_dir + '/test_coco_0_annotation_{0}.json'.format(n),'w') as json_file:\n",
    "        json.dump(test_coco_0_annotation, json_file)\n",
    "\n",
    "    #save coco 1 annotations in new dir\n",
    "    with open(coco_anno_1_dir + '/train_coco_1_annotation_{0}.json'.format(n),'w') as json_file:\n",
    "        json.dump(train_coco_1_annotation, json_file)\n",
    "    with open(coco_anno_1_dir + '/val_coco_1_annotation_{0}.json'.format(n),'w') as json_file:\n",
    "        json.dump(val_coco_1_annotation, json_file)\n",
    "    with open(coco_anno_1_dir + '/test_coco_1_annotation_{0}.json'.format(n),'w') as json_file:\n",
    "        json.dump(test_coco_1_annotation, json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b545ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save indices of train, validation and test set\n",
    "\n",
    "def save_index_lists(root_dir, new_dir, indices):\n",
    "        \n",
    "    cwd = os.getcwd()\n",
    "    dir = os.path.join(cwd, root_dir, new_dir)\n",
    "    \n",
    "    pickle_dir = os.path.join(dir, \"pickled_index_lists\")\n",
    "    if not os.path.exists(pickle_dir):\n",
    "        os.mkdir(pickle_dir)\n",
    "\n",
    "    # store train_ind\n",
    "    with open(pickle_dir + '/train_index', 'wb') as fp:\n",
    "        pickle.dump(indices[0], fp)\n",
    "        print('Done writing train index list into a binary file')\n",
    "    \n",
    "    # store val_ind\n",
    "    with open(pickle_dir + '/val_index', 'wb') as fp:\n",
    "        pickle.dump(indices[1], fp)\n",
    "        print('Done writing val index list into a binary file')\n",
    "        \n",
    "    # store test_ind\n",
    "    with open(pickle_dir + '/test_index', 'wb') as fp:\n",
    "        pickle.dump(indices[2], fp)\n",
    "        print('Done writing test index list into a binary file')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ecdc068",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dir = \"data_{0}_samples\".format(n)\n",
    "\n",
    "save_annotations(root_dir, new_dir, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff2e6a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing train index list into a binary file\n",
      "Done writing val index list into a binary file\n",
      "Done writing test index list into a binary file\n"
     ]
    }
   ],
   "source": [
    "save_index_lists(root_dir, new_dir, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7511f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data\n",
    "\n",
    "# with open(dir + '/train_coco_annotation_{0}.json'.format(n), 'r') as file:\n",
    "#      train_coco_annotation = json.load(file) \n",
    "# with open(dir + '/val_coco_annotation_{0}.json'.format(n), 'r') as file:\n",
    "#      val_coco_annotation = json.load(file)\n",
    "# with open(dir + '/test_coco_annotation_{0}.json'.format(n), 'r') as file:\n",
    "#      test_coco_annotation = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f7805f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check pickled list\n",
    "\n",
    "# cwd = os.getcwd()\n",
    "# path = os.path.join(cwd, root_dir, new_dir, \"pickled_index_lists\" )\n",
    "# with open(path + '/val_index', 'rb') as fp:\n",
    "#     n_list = pickle.load(fp)\n",
    "    \n",
    "# n_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
